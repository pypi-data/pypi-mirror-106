import click
import neuro_sdk
import signal
import sys
from typing import List, Optional, Sequence, Tuple

from neuro_flow.batch_executor import ExecutorData
from neuro_flow.batch_runner import BatchRunner
from neuro_flow.cli.click_types import (
    BAKE,
    BATCH,
    BATCH_OR_ALL,
    FINISHED_TASK_AFTER_BAKE,
)
from neuro_flow.cli.utils import argument, option, resolve_bake, wrap_async
from neuro_flow.storage import APIStorage, NeuroStorageFS, Storage
from neuro_flow.types import LocalPath

from ..parser import parse_bake_meta
from .root import Root


if sys.version_info >= (3, 7):
    from contextlib import AsyncExitStack
else:
    from async_exit_stack import AsyncExitStack


@click.command()  # type: ignore
@option("--local-executor", is_flag=True, default=False, help="Run primary job locally")
@click.option(  # type: ignore
    "--param", type=(str, str), multiple=True, help="Set params of the batch config"
)
@click.option(  # type: ignore
    "-n",
    "--name",
    metavar="NAME",
    type=str,
    help="Optional bake name",
    default=None,
)
@click.option(  # type: ignore
    "--meta-from-file",
    type=click.Path(  # type: ignore
        exists=True, file_okay=True, dir_okay=False, readable=True
    ),
    help="File with params for batch.",
)
@click.option(  # type: ignore
    "-t",
    "--tag",
    metavar="TAG",
    type=str,
    help="Optional bake tag, multiple values allowed",
    multiple=True,
)
@argument("batch", type=BATCH)
@wrap_async()
async def bake(
    root: Root,
    batch: str,
    local_executor: bool,
    meta_from_file: Optional[str],
    param: List[Tuple[str, str]],
    name: Optional[str],
    tag: Sequence[str],
) -> None:
    """Start a batch.

    Run BATCH pipeline remotely on the cluster.
    """
    async with AsyncExitStack() as stack:
        client = await stack.enter_async_context(neuro_sdk.get())
        storage: Storage = await stack.enter_async_context(
            APIStorage(client, NeuroStorageFS(client))
        )
        runner = await stack.enter_async_context(
            BatchRunner(root.config_dir, root.console, client, storage, root)
        )
        params = {key: value for key, value in param}
        if meta_from_file is not None:
            bake_meta = parse_bake_meta(LocalPath(meta_from_file))
            params = {**bake_meta, **params}
        await runner.bake(
            batch_name=batch,
            local_executor=local_executor,
            params=params,
            name=name,
            tags=tag,
        )


@click.command(hidden=True)  # type: ignore
@click.argument("executor_data")  # type: ignore
@wrap_async()
async def execute(
    root: Root,
    executor_data: str,
) -> None:
    """Start a batch.

    Run BATCH pipeline remotely on the cluster.
    """
    # neuro-flow execute is run in linux container only,
    # Linux signals are always defined.
    for signame in (
        signal.SIGHUP,
        signal.SIGINT,
        signal.SIGQUIT,
        signal.SIGTSTP,
        signal.SIGTERM,
        signal.SIGTTIN,
        signal.SIGTTOU,
        signal.SIGWINCH,
    ):
        # ignore everything, use neuro-flow cancel to stop the master job.
        signal.signal(signame, signal.SIG_IGN)
    async with AsyncExitStack() as stack:
        data = ExecutorData.parse(executor_data)
        client = await stack.enter_async_context(neuro_sdk.get())
        storage: Storage = await stack.enter_async_context(
            APIStorage(client, NeuroStorageFS(client))
        )
        runner = await stack.enter_async_context(
            BatchRunner(root.config_dir, root.console, client, storage, root)
        )
        await runner.process(data)


@click.command()  # type: ignore
@click.option(  # type: ignore
    "-t",
    "--tag",
    metavar="TAG",
    type=str,
    help="Filter out bakes by tag (multiple option)",
    multiple=True,
)
@wrap_async()
async def bakes(
    root: Root,
    tag: Sequence[str],
) -> None:
    """List existing bakes."""
    async with AsyncExitStack() as stack:
        client = await stack.enter_async_context(neuro_sdk.get())
        storage: Storage = await stack.enter_async_context(
            APIStorage(client, NeuroStorageFS(client))
        )
        runner = await stack.enter_async_context(
            BatchRunner(root.config_dir, root.console, client, storage, root)
        )
        await runner.list_bakes(set(tag))


@click.command()  # type: ignore
@argument("bake", type=BAKE)
@option(
    "-a",
    "--attempt",
    type=int,
    default=-1,
    help="Attempt number, the last attempt by default",
)
@click.option(  # type: ignore
    "-o",
    "--output-graph",
    type=click.Path(file_okay=True, dir_okay=False, writable=True),  # type: ignore
    help=(
        "A path to Graphviz (DOT) file. "
        "Autogenerated from BAKE_ID and attempt number by default"
    ),
)
@click.option(  # type: ignore
    "--dot",
    is_flag=True,
    help=("Save DOT file with tasks statuses."),
)
@click.option(  # type: ignore
    "--pdf",
    is_flag=True,
    help=("Save PDF file with tasks statuses."),
)
@click.option(  # type: ignore
    "--view",
    is_flag=True,
    help=("Open generated PDF file with tasks statuses."),
)
@wrap_async()
async def inspect(
    root: Root,
    bake: str,
    attempt: int,
    output_graph: Optional[str],
    dot: bool,
    pdf: bool,
    view: bool,
) -> None:
    """Inspect a bake.

    Display a list of started/finished tasks of BAKE\\_ID.
    """
    async with AsyncExitStack() as stack:
        client = await stack.enter_async_context(neuro_sdk.get())
        storage: Storage = await stack.enter_async_context(
            APIStorage(client, NeuroStorageFS(client))
        )
        runner = await stack.enter_async_context(
            BatchRunner(root.config_dir, root.console, client, storage, root)
        )
        if output_graph is not None:
            real_output: Optional[LocalPath] = LocalPath(output_graph)
        else:
            real_output = None
        bake_id = await resolve_bake(bake, project=runner.project, storage=storage)

        await runner.inspect(
            bake_id,
            attempt_no=attempt,
            output=real_output,
            save_dot=dot,
            save_pdf=pdf,
            view_pdf=view,
        )


@click.command()  # type: ignore
@argument("bake", type=BAKE)
@argument("task_id", type=FINISHED_TASK_AFTER_BAKE)
@click.option(  # type: ignore
    "-a",
    "--attempt",
    type=int,
    default=-1,
    help="Attempt number, the last attempt by default",
)
@click.option(  # type: ignore
    "-r/-R",
    "--raw/--no-raw",
    default=False,
    help=(
        "Raw mode disables the output postprocessing "
        "(the output is processed by default)"
    ),
)
@wrap_async()
async def show(
    root: Root,
    bake: str,
    attempt: int,
    task_id: str,
    raw: bool,
) -> None:
    """Show output of baked task.

    Display a logged output of TASK\\_ID from BAKE\\_ID.
    """
    async with AsyncExitStack() as stack:
        client = await stack.enter_async_context(neuro_sdk.get())
        storage: Storage = await stack.enter_async_context(
            APIStorage(client, NeuroStorageFS(client))
        )
        runner = await stack.enter_async_context(
            BatchRunner(root.config_dir, root.console, client, storage, root)
        )
        bake_id = await resolve_bake(bake, project=runner.project, storage=storage)
        await runner.logs(bake_id, task_id, attempt_no=attempt, raw=raw)


@click.command()  # type: ignore
@argument("bake", type=BAKE)
@click.option(  # type: ignore
    "-a",
    "--attempt",
    type=int,
    default=-1,
    help="Attempt number, the last attempt by default",
)
@wrap_async()
async def cancel(
    root: Root,
    bake: str,
    attempt: int,
) -> None:
    """Cancel a bake.

    Cancel a bake execution by stopping all started tasks.
    """
    async with AsyncExitStack() as stack:
        client = await stack.enter_async_context(neuro_sdk.get())
        storage: Storage = await stack.enter_async_context(
            APIStorage(client, NeuroStorageFS(client))
        )
        runner = await stack.enter_async_context(
            BatchRunner(root.config_dir, root.console, client, storage, root)
        )
        bake_id = await resolve_bake(bake, project=runner.project, storage=storage)
        await runner.cancel(bake_id, attempt_no=attempt)


@click.command()  # type: ignore
@argument("batch", type=BATCH_OR_ALL)
@wrap_async()
async def clear_cache(
    root: Root,
    batch: str,
) -> None:
    """Clear cache.

    Use `neuro-flow clear-cache <BATCH>` for cleaning up the cache for BATCH;

    `neuro-flow clear-cache ALL` clears all caches.
    """
    async with AsyncExitStack() as stack:
        client = await stack.enter_async_context(neuro_sdk.get())
        storage: Storage = await stack.enter_async_context(
            APIStorage(client, NeuroStorageFS(client))
        )
        runner = await stack.enter_async_context(
            BatchRunner(root.config_dir, root.console, client, storage, root)
        )
        if batch == "ALL":
            await runner.clear_cache(None)
        else:
            await runner.clear_cache(batch)


@click.command()  # type: ignore
@argument("bake", type=BAKE)
@option(
    "-a",
    "--attempt",
    type=int,
    default=-1,
    help="Attempt number, the last attempt by default",
)
@option("--local-executor", is_flag=True, default=False, help="Run primary job locally")
@option(
    "--from-failed/--no-from-failed",
    is_flag=True,
    default=True,
    help="Restart from the point of failure",
)
@wrap_async()
async def restart(
    root: Root,
    bake: str,
    attempt: int,
    from_failed: bool,
    local_executor: bool,
) -> None:
    """Start a batch.

    Run BATCH pipeline remotely on the cluster.
    """
    async with AsyncExitStack() as stack:
        client = await stack.enter_async_context(neuro_sdk.get())
        storage: Storage = await stack.enter_async_context(
            APIStorage(client, NeuroStorageFS(client))
        )
        runner = await stack.enter_async_context(
            BatchRunner(root.config_dir, root.console, client, storage, root)
        )
        bake_id = await resolve_bake(bake, project=runner.project, storage=storage)
        await runner.restart(
            bake_id,
            attempt_no=attempt,
            from_failed=from_failed,
            local_executor=local_executor,
        )
